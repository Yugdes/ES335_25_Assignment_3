{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2: Linux Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Memory: 85.03 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url=\"https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt\"):\n",
    "    \"\"\"Download and load the Linux kernel code dataset\"\"\"\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    print(f\"Dataset loaded: {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, vocab_size=100000):\n",
    "    \"\"\"\n",
    "    Preprocess text for code (preserve special characters)\n",
    "    Split by newlines to treat each line as a statement\n",
    "    \"\"\"\n",
    "    print(\"\\nPreprocessing text...\")\n",
    "    \n",
    "    # Split into lines and filter empty lines\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Tokenize into words (keeping code-relevant symbols)\n",
    "    all_words = []\n",
    "    for line in lines:\n",
    "        # Split on whitespace but keep the structure\n",
    "        words = line.split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    print(f\"Total words: {len(all_words)}\")\n",
    "    \n",
    "    # Build vocabulary from most frequent words\n",
    "    word_counts = Counter(all_words)\n",
    "    most_common = word_counts.most_common(vocab_size - 1)  # -1 for <UNK> token\n",
    "    \n",
    "    vocab = ['<UNK>'] + [word for word, _ in most_common]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "    print(f\"\\n10 most frequent words:\")\n",
    "    for word, count in most_common[:10]:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    print(f\"\\n10 least frequent words in vocabulary:\")\n",
    "    for word, count in most_common[-10:]:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    return all_words, vocab, word_to_idx, idx_to_word\n",
    "\n",
    "def create_sequences(words, word_to_idx, context_length):\n",
    "    \"\"\"Create input-output sequences for training\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(words) - context_length):\n",
    "        context = words[i:i + context_length]\n",
    "        target = words[i + context_length]\n",
    "        \n",
    "        # Convert to indices (use <UNK> for unknown words)\n",
    "        context_indices = [word_to_idx.get(w, 0) for w in context]\n",
    "        target_idx = word_to_idx.get(target, 0)\n",
    "        \n",
    "        X.append(context_indices)\n",
    "        y.append(target_idx)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPredictionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, \n",
    "                 hidden_size=1024, activation='relu'):\n",
    "        super(NextWordMLP, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten_size = context_length * embedding_dim\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_length = context_length\n",
    "        self.activation_name = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, context_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, context_length, embedding_dim)\n",
    "        flattened = embedded.view(embedded.size(0), -1)  # (batch_size, context_length * embedding_dim)\n",
    "        \n",
    "        hidden = self.activation(self.fc1(flattened))\n",
    "        output = self.fc2(hidden)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, lr, device, model_name):\n",
    "    \"\"\"Train a single model\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_accuracy = 100 * correct / total\n",
    "        \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFinal Results - Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SAVE MODEL AND METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifacts(model, history, vocab, word_to_idx, idx_to_word, \n",
    "                        config, model_name, save_dir='models'):\n",
    "    \"\"\"Save model, weights, vocabulary, and training history\"\"\"\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model_path = f\"{save_dir}/{model_name}_weights.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save vocabulary and mappings\n",
    "    vocab_data = {\n",
    "        'vocab': vocab,\n",
    "        'word_to_idx': word_to_idx,\n",
    "        'idx_to_word': idx_to_word\n",
    "    }\n",
    "    with open(f\"{save_dir}/{model_name}_vocab.pkl\", 'wb') as f:\n",
    "        pickle.dump(vocab_data, f)\n",
    "    \n",
    "    # Save config\n",
    "    with open(f\"{save_dir}/{model_name}_config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save training history\n",
    "    with open(f\"{save_dir}/{model_name}_history.pkl\", 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    print(f\"Saved artifacts for {model_name}\")\n",
    "\n",
    "def plot_training_history(history, model_name, save_dir='models'):\n",
    "    \"\"\"Plot and save training curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(history['train_losses'], label='Train Loss')\n",
    "    ax1.plot(history['val_losses'], label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{model_name} - Loss Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy curve\n",
    "    ax2.plot(history['val_accuracies'], label='Val Accuracy', color='green')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title(f'{model_name} - Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/{model_name}_curves.png\", dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MAIN TRAINING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameter configurations\n",
    "    configs = [\n",
    "        # context_length, embedding_dim, activation\n",
    "        (3, 32, 'relu'),\n",
    "        (3, 32, 'tanh'),\n",
    "        (3, 64, 'relu'),\n",
    "        (3, 64, 'tanh'),\n",
    "        (5, 32, 'relu'),\n",
    "        (5, 32, 'tanh'),\n",
    "        (5, 64, 'relu'),\n",
    "        (5, 64, 'tanh'),\n",
    "    ]\n",
    "    \n",
    "    # Training parameters\n",
    "    VOCAB_SIZE = 100000\n",
    "    HIDDEN_SIZE = 1024\n",
    "    EPOCHS = 200\n",
    "    BATCH_SIZE = 512  # Large batch for H100\n",
    "    LEARNING_RATE = 0.001\n",
    "    VAL_SPLIT = 0.1\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    text = load_data()\n",
    "    all_words, vocab, word_to_idx, idx_to_word = preprocess_text(text, VOCAB_SIZE)\n",
    "    \n",
    "    # Train each model configuration\n",
    "    for idx, (context_length, embedding_dim, activation) in enumerate(configs, 1):\n",
    "        model_name = f\"model_{idx}_ctx{context_length}_emb{embedding_dim}_{activation}\"\n",
    "        \n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"# MODEL {idx}/8: Context={context_length}, Embedding={embedding_dim}, Activation={activation}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        # Create sequences for this context length\n",
    "        print(f\"\\nCreating sequences with context length {context_length}...\")\n",
    "        X, y = create_sequences(all_words, word_to_idx, context_length)\n",
    "        print(f\"Total sequences: {len(X)}\")\n",
    "        \n",
    "        # Train-validation split\n",
    "        split_idx = int(len(X) * (1 - VAL_SPLIT))\n",
    "        X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "        X_val, y_val = X[split_idx:], y[split_idx:]\n",
    "        \n",
    "        print(f\"Train samples: {len(X_train)}, Val samples: {len(X_val)}\")\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = WordPredictionDataset(X_train, y_train)\n",
    "        val_dataset = WordPredictionDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                                 shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                               shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        # Create model\n",
    "        model = NextWordMLP(\n",
    "            vocab_size=len(vocab),\n",
    "            embedding_dim=embedding_dim,\n",
    "            context_length=context_length,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel architecture:\")\n",
    "        print(f\"  Vocab size: {len(vocab)}\")\n",
    "        print(f\"  Embedding dim: {embedding_dim}\")\n",
    "        print(f\"  Context length: {context_length}\")\n",
    "        print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "        print(f\"  Activation: {activation}\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Train model\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader, \n",
    "            EPOCHS, LEARNING_RATE, device, model_name\n",
    "        )\n",
    "        \n",
    "        # Save model and artifacts\n",
    "        config = {\n",
    "            'context_length': context_length,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'activation': activation,\n",
    "            'vocab_size': len(vocab),\n",
    "            'hidden_size': HIDDEN_SIZE,\n",
    "            'epochs': EPOCHS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'final_val_loss': history['val_losses'][-1],\n",
    "            'final_val_accuracy': history['val_accuracies'][-1]\n",
    "        }\n",
    "        \n",
    "        save_model_artifacts(\n",
    "            model, history, vocab, word_to_idx, idx_to_word,\n",
    "            config, model_name\n",
    "        )\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_history(history, model_name)\n",
    "        \n",
    "        # Generate sample predictions\n",
    "        print(f\"\\nSample predictions for {model_name}:\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample_idx = np.random.randint(0, len(X_val), 3)\n",
    "            for i in sample_idx:\n",
    "                context = X_val[i]\n",
    "                true_next = y_val[i]\n",
    "                \n",
    "                context_words = [idx_to_word[idx] for idx in context]\n",
    "                true_word = idx_to_word[true_next]\n",
    "                \n",
    "                # Predict\n",
    "                context_tensor = torch.LongTensor([context]).to(device)\n",
    "                output = model(context_tensor)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predicted_word = idx_to_word[predicted.item()]\n",
    "                \n",
    "                print(f\"  Context: {' '.join(context_words)}\")\n",
    "                print(f\"  True: {true_word} | Predicted: {predicted_word}\")\n",
    "                print()\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del model, train_loader, val_loader, train_dataset, val_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL 8 MODELS TRAINED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nSaved files in 'models/' directory:\")\n",
    "    print(\"  - model_*_weights.pth (model weights)\")\n",
    "    print(\"  - model_*_vocab.pkl (vocabulary)\")\n",
    "    print(\"  - model_*_config.json (configuration)\")\n",
    "    print(\"  - model_*_history.pkl (training history)\")\n",
    "    print(\"  - model_*_curves.png (loss/accuracy plots)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset loaded: 6206996 characters\n",
      "\n",
      "Preprocessing text...\n",
      "Total words: 759639\n",
      "\n",
      "Vocabulary size: 100000\n",
      "\n",
      "10 most frequent words:\n",
      "  *: 33504\n",
      "  =: 28003\n",
      "  {: 18915\n",
      "  if: 17702\n",
      "  }: 16965\n",
      "  the: 16080\n",
      "  */: 13445\n",
      "  /*: 12190\n",
      "  struct: 10997\n",
      "  return: 10130\n",
      "\n",
      "10 least frequent words in vocabulary:\n",
      "  AUDIT_MMAP:: 1\n",
      "  \"fd=%d: 1\n",
      "  flags=0x%x\",: 1\n",
      "  context->mmap.fd,: 1\n",
      "  context->mmap.flags);: 1\n",
      "  AUDIT_EXECVE:: 1\n",
      "  audit_log_execve_info(context,: 1\n",
      "  &ab);: 1\n",
      "  audit_proctitle_rtrim(char: 1\n",
      "  *proctitle,: 1\n",
      "\n",
      "############################################################\n",
      "# MODEL 1/8: Context=3, Embedding=32, Activation=relu\n",
      "############################################################\n",
      "\n",
      "Creating sequences with context length 3...\n",
      "Total sequences: 759636\n",
      "Train samples: 683672, Val samples: 75964\n",
      "\n",
      "Model architecture:\n",
      "  Vocab size: 100000\n",
      "  Embedding dim: 32\n",
      "  Context length: 3\n",
      "  Hidden size: 1024\n",
      "  Activation: relu\n",
      "  Total parameters: 105,799,328\n",
      "\n",
      "============================================================\n",
      "Training: model_1_ctx3_emb32_relu\n",
      "============================================================\n",
      "Epoch [1/200] - Train Loss: 6.8839, Val Loss: 6.1506, Val Acc: 19.19%\n",
      "Epoch [10/200] - Train Loss: 1.6533, Val Loss: 9.2180, Val Acc: 22.55%\n",
      "Epoch [20/200] - Train Loss: 1.1006, Val Loss: 11.1405, Val Acc: 21.65%\n",
      "Epoch [30/200] - Train Loss: 0.8918, Val Loss: 12.6443, Val Acc: 21.73%\n",
      "Epoch [40/200] - Train Loss: 0.7911, Val Loss: 13.9050, Val Acc: 21.10%\n",
      "Epoch [50/200] - Train Loss: 0.7340, Val Loss: 15.0943, Val Acc: 20.87%\n",
      "Epoch [60/200] - Train Loss: 0.6962, Val Loss: 16.1406, Val Acc: 20.79%\n",
      "Epoch [70/200] - Train Loss: 0.6703, Val Loss: 17.0926, Val Acc: 20.52%\n",
      "Epoch [80/200] - Train Loss: 0.6510, Val Loss: 18.0232, Val Acc: 19.97%\n",
      "Epoch [90/200] - Train Loss: 0.6355, Val Loss: 18.8645, Val Acc: 19.80%\n",
      "Epoch [100/200] - Train Loss: 0.6249, Val Loss: 19.7222, Val Acc: 20.08%\n",
      "Epoch [110/200] - Train Loss: 0.6152, Val Loss: 20.5480, Val Acc: 19.63%\n",
      "Epoch [120/200] - Train Loss: 0.6073, Val Loss: 21.3408, Val Acc: 19.93%\n",
      "Epoch [130/200] - Train Loss: 0.6008, Val Loss: 22.0662, Val Acc: 19.42%\n",
      "Epoch [140/200] - Train Loss: 0.5950, Val Loss: 22.7074, Val Acc: 19.77%\n",
      "Epoch [150/200] - Train Loss: 0.5904, Val Loss: 23.4892, Val Acc: 19.31%\n",
      "Epoch [160/200] - Train Loss: 0.5866, Val Loss: 24.0337, Val Acc: 19.08%\n",
      "Epoch [170/200] - Train Loss: 0.5826, Val Loss: 24.5707, Val Acc: 18.84%\n",
      "Epoch [180/200] - Train Loss: 0.5794, Val Loss: 25.2180, Val Acc: 18.93%\n",
      "Epoch [190/200] - Train Loss: 0.5766, Val Loss: 25.8448, Val Acc: 19.01%\n",
      "Epoch [200/200] - Train Loss: 0.5741, Val Loss: 26.2980, Val Acc: 19.33%\n",
      "\n",
      "Final Results - Val Loss: 26.2980, Val Acc: 19.33%\n",
      "Saved artifacts for model_1_ctx3_emb32_relu\n",
      "\n",
      "Sample predictions for model_1_ctx3_emb32_relu:\n",
      "  Context: */ int __weak\n",
      "  True: <UNK> | Predicted: <UNK>\n",
      "\n",
      "  Context: %s - tainting\n",
      "  True: <UNK> | Predicted: will\n",
      "\n",
      "  Context: -1; blk_trace_setup_lba(bt, bdev);\n",
      "  True: /* | Predicted: LOAD_FREQ;\n",
      "\n",
      "\n",
      "############################################################\n",
      "# MODEL 2/8: Context=3, Embedding=32, Activation=tanh\n",
      "############################################################\n",
      "\n",
      "Creating sequences with context length 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8239/2624463566.py:114: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  context_tensor = torch.LongTensor([context]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 759636\n",
      "Train samples: 683672, Val samples: 75964\n",
      "\n",
      "Model architecture:\n",
      "  Vocab size: 100000\n",
      "  Embedding dim: 32\n",
      "  Context length: 3\n",
      "  Hidden size: 1024\n",
      "  Activation: tanh\n",
      "  Total parameters: 105,799,328\n",
      "\n",
      "============================================================\n",
      "Training: model_2_ctx3_emb32_tanh\n",
      "============================================================\n",
      "Epoch [1/200] - Train Loss: 6.9371, Val Loss: 6.5403, Val Acc: 17.30%\n",
      "Epoch [10/200] - Train Loss: 2.7366, Val Loss: 7.9863, Val Acc: 20.79%\n",
      "Epoch [20/200] - Train Loss: 1.9241, Val Loss: 9.0893, Val Acc: 20.75%\n",
      "Epoch [30/200] - Train Loss: 1.5757, Val Loss: 9.9596, Val Acc: 19.98%\n",
      "Epoch [40/200] - Train Loss: 1.4122, Val Loss: 10.4973, Val Acc: 19.01%\n",
      "Epoch [50/200] - Train Loss: 1.3199, Val Loss: 10.9529, Val Acc: 19.04%\n",
      "Epoch [60/200] - Train Loss: 1.2578, Val Loss: 11.3230, Val Acc: 18.14%\n",
      "Epoch [70/200] - Train Loss: 1.2140, Val Loss: 11.6303, Val Acc: 18.00%\n",
      "Epoch [80/200] - Train Loss: 1.1807, Val Loss: 11.8812, Val Acc: 17.30%\n",
      "Epoch [90/200] - Train Loss: 1.1519, Val Loss: 12.1529, Val Acc: 17.83%\n",
      "Epoch [100/200] - Train Loss: 1.1284, Val Loss: 12.3473, Val Acc: 17.60%\n",
      "Epoch [110/200] - Train Loss: 1.1097, Val Loss: 12.6111, Val Acc: 18.12%\n",
      "Epoch [120/200] - Train Loss: 1.0931, Val Loss: 12.8420, Val Acc: 17.88%\n",
      "Epoch [130/200] - Train Loss: 1.0792, Val Loss: 13.1071, Val Acc: 17.82%\n",
      "Epoch [140/200] - Train Loss: 1.0668, Val Loss: 13.3203, Val Acc: 17.38%\n",
      "Epoch [150/200] - Train Loss: 1.0536, Val Loss: 13.5841, Val Acc: 17.37%\n",
      "Epoch [160/200] - Train Loss: 1.0438, Val Loss: 13.7818, Val Acc: 17.59%\n",
      "Epoch [170/200] - Train Loss: 1.0343, Val Loss: 13.9905, Val Acc: 17.61%\n",
      "Epoch [180/200] - Train Loss: 1.0261, Val Loss: 14.2075, Val Acc: 17.63%\n",
      "Epoch [190/200] - Train Loss: 1.0187, Val Loss: 14.4741, Val Acc: 16.91%\n",
      "Epoch [200/200] - Train Loss: 1.0115, Val Loss: 14.6428, Val Acc: 16.83%\n",
      "\n",
      "Final Results - Val Loss: 14.6428, Val Acc: 16.83%\n",
      "Saved artifacts for model_2_ctx3_emb32_tanh\n",
      "\n",
      "Sample predictions for model_2_ctx3_emb32_tanh:\n",
      "  Context: file in sysfs.\n",
      "  True: Returns | Predicted: Don't\n",
      "\n",
      "  Context: __trace_stack(tr, flags, 5,\n",
      "  True: pc); | Predicted: pc);\n",
      "\n",
      "  Context: if (diag) return\n",
      "  True: diag; | Predicted: diag;\n",
      "\n",
      "\n",
      "############################################################\n",
      "# MODEL 3/8: Context=3, Embedding=64, Activation=relu\n",
      "############################################################\n",
      "\n",
      "Creating sequences with context length 3...\n",
      "Total sequences: 759636\n",
      "Train samples: 683672, Val samples: 75964\n",
      "\n",
      "Model architecture:\n",
      "  Vocab size: 100000\n",
      "  Embedding dim: 64\n",
      "  Context length: 3\n",
      "  Hidden size: 1024\n",
      "  Activation: relu\n",
      "  Total parameters: 109,097,632\n",
      "\n",
      "============================================================\n",
      "Training: model_3_ctx3_emb64_relu\n",
      "============================================================\n",
      "Epoch [1/200] - Train Loss: 6.7476, Val Loss: 6.0840, Val Acc: 20.91%\n",
      "Epoch [10/200] - Train Loss: 1.3875, Val Loss: 9.5976, Val Acc: 22.64%\n",
      "Epoch [20/200] - Train Loss: 0.8844, Val Loss: 12.2582, Val Acc: 21.76%\n",
      "Epoch [30/200] - Train Loss: 0.7390, Val Loss: 14.2072, Val Acc: 21.74%\n",
      "Epoch [40/200] - Train Loss: 0.6776, Val Loss: 15.7135, Val Acc: 21.49%\n",
      "Epoch [50/200] - Train Loss: 0.6451, Val Loss: 16.9669, Val Acc: 21.49%\n",
      "Epoch [60/200] - Train Loss: 0.6228, Val Loss: 18.1572, Val Acc: 21.08%\n",
      "Epoch [70/200] - Train Loss: 0.6085, Val Loss: 19.2709, Val Acc: 20.85%\n",
      "Epoch [80/200] - Train Loss: 0.5958, Val Loss: 20.3275, Val Acc: 20.94%\n",
      "Epoch [90/200] - Train Loss: 0.5861, Val Loss: 21.4383, Val Acc: 20.69%\n",
      "Epoch [100/200] - Train Loss: 0.5778, Val Loss: 22.4883, Val Acc: 20.50%\n",
      "Epoch [110/200] - Train Loss: 0.5712, Val Loss: 23.5419, Val Acc: 20.36%\n",
      "Epoch [120/200] - Train Loss: 0.5654, Val Loss: 24.4331, Val Acc: 20.27%\n",
      "Epoch [130/200] - Train Loss: 0.5603, Val Loss: 25.6234, Val Acc: 19.98%\n",
      "Epoch [140/200] - Train Loss: 0.5554, Val Loss: 26.4338, Val Acc: 19.82%\n",
      "Epoch [150/200] - Train Loss: 0.5524, Val Loss: 27.4711, Val Acc: 19.95%\n",
      "Epoch [160/200] - Train Loss: 0.5484, Val Loss: 28.4296, Val Acc: 19.85%\n",
      "Epoch [170/200] - Train Loss: 0.5457, Val Loss: 29.5367, Val Acc: 19.60%\n",
      "Epoch [180/200] - Train Loss: 0.5429, Val Loss: 30.4211, Val Acc: 19.60%\n",
      "Epoch [190/200] - Train Loss: 0.5408, Val Loss: 31.4176, Val Acc: 19.42%\n",
      "Epoch [200/200] - Train Loss: 0.5383, Val Loss: 32.5452, Val Acc: 19.45%\n",
      "\n",
      "Final Results - Val Loss: 32.5452, Val Acc: 19.45%\n",
      "Saved artifacts for model_3_ctx3_emb64_relu\n",
      "\n",
      "Sample predictions for model_3_ctx3_emb64_relu:\n",
      "  Context: (argc >= MAXARGC\n",
      "  True: - | Predicted: ret\n",
      "\n",
      "  Context: trace_dump_stack(STACK_SKIP); } static\n",
      "  True: void | Predicted: void\n",
      "\n",
      "  Context: break; default: ret\n",
      "  True: = | Predicted: =\n",
      "\n",
      "\n",
      "############################################################\n",
      "# MODEL 4/8: Context=3, Embedding=64, Activation=tanh\n",
      "############################################################\n",
      "\n",
      "Creating sequences with context length 3...\n",
      "Total sequences: 759636\n",
      "Train samples: 683672, Val samples: 75964\n",
      "\n",
      "Model architecture:\n",
      "  Vocab size: 100000\n",
      "  Embedding dim: 64\n",
      "  Context length: 3\n",
      "  Hidden size: 1024\n",
      "  Activation: tanh\n",
      "  Total parameters: 109,097,632\n",
      "\n",
      "============================================================\n",
      "Training: model_4_ctx3_emb64_tanh\n",
      "============================================================\n",
      "Epoch [1/200] - Train Loss: 6.7970, Val Loss: 6.4734, Val Acc: 18.43%\n",
      "Epoch [10/200] - Train Loss: 2.3505, Val Loss: 8.3049, Val Acc: 21.75%\n",
      "Epoch [20/200] - Train Loss: 1.6538, Val Loss: 9.2989, Val Acc: 20.24%\n",
      "Epoch [30/200] - Train Loss: 1.3692, Val Loss: 9.8685, Val Acc: 19.50%\n",
      "Epoch [40/200] - Train Loss: 1.2357, Val Loss: 10.2438, Val Acc: 18.96%\n",
      "Epoch [50/200] - Train Loss: 1.1597, Val Loss: 10.5611, Val Acc: 18.56%\n",
      "Epoch [60/200] - Train Loss: 1.1107, Val Loss: 10.8467, Val Acc: 18.65%\n",
      "Epoch [70/200] - Train Loss: 1.0715, Val Loss: 11.1363, Val Acc: 18.03%\n",
      "Epoch [80/200] - Train Loss: 1.0427, Val Loss: 11.4085, Val Acc: 18.01%\n",
      "Epoch [90/200] - Train Loss: 1.0186, Val Loss: 11.6187, Val Acc: 17.93%\n",
      "Epoch [100/200] - Train Loss: 0.9999, Val Loss: 11.8529, Val Acc: 17.58%\n",
      "Epoch [110/200] - Train Loss: 0.9806, Val Loss: 12.0594, Val Acc: 17.42%\n",
      "Epoch [120/200] - Train Loss: 0.9649, Val Loss: 12.2437, Val Acc: 17.43%\n",
      "Epoch [130/200] - Train Loss: 0.9544, Val Loss: 12.4479, Val Acc: 17.11%\n",
      "Epoch [140/200] - Train Loss: 0.9423, Val Loss: 12.6332, Val Acc: 17.44%\n",
      "Epoch [150/200] - Train Loss: 0.9328, Val Loss: 12.7986, Val Acc: 17.39%\n",
      "Epoch [160/200] - Train Loss: 0.9226, Val Loss: 13.0021, Val Acc: 16.85%\n",
      "Epoch [170/200] - Train Loss: 0.9157, Val Loss: 13.1304, Val Acc: 16.86%\n",
      "Epoch [180/200] - Train Loss: 0.9084, Val Loss: 13.3178, Val Acc: 17.09%\n",
      "Epoch [190/200] - Train Loss: 0.9020, Val Loss: 13.4973, Val Acc: 16.68%\n",
      "Epoch [200/200] - Train Loss: 0.8953, Val Loss: 13.6220, Val Acc: 16.94%\n",
      "\n",
      "Final Results - Val Loss: 13.6220, Val Acc: 16.94%\n",
      "Saved artifacts for model_4_ctx3_emb64_tanh\n",
      "\n",
      "Sample predictions for model_4_ctx3_emb64_tanh:\n",
      "  Context: }; static const\n",
      "  True: struct | Predicted: struct\n",
      "\n",
      "  Context: sequence (serial) number\n",
      "  True: */ | Predicted: =\n",
      "\n",
      "  Context: { bool <UNK>\n",
      "  True: = | Predicted: =\n",
      "\n",
      "\n",
      "############################################################\n",
      "# MODEL 5/8: Context=5, Embedding=32, Activation=relu\n",
      "############################################################\n",
      "\n",
      "Creating sequences with context length 5...\n",
      "Total sequences: 759634\n",
      "Train samples: 683670, Val samples: 75964\n",
      "\n",
      "Model architecture:\n",
      "  Vocab size: 100000\n",
      "  Embedding dim: 32\n",
      "  Context length: 5\n",
      "  Hidden size: 1024\n",
      "  Activation: relu\n",
      "  Total parameters: 105,864,864\n",
      "\n",
      "============================================================\n",
      "Training: model_5_ctx5_emb32_relu\n",
      "============================================================\n",
      "Epoch [1/200] - Train Loss: 6.8780, Val Loss: 6.0718, Val Acc: 21.22%\n",
      "Epoch [10/200] - Train Loss: 1.1600, Val Loss: 9.9444, Val Acc: 23.26%\n",
      "Epoch [20/200] - Train Loss: 0.5747, Val Loss: 12.5658, Val Acc: 21.61%\n",
      "Epoch [30/200] - Train Loss: 0.3505, Val Loss: 14.8236, Val Acc: 20.97%\n",
      "Epoch [40/200] - Train Loss: 0.2484, Val Loss: 16.7939, Val Acc: 20.44%\n",
      "Epoch [50/200] - Train Loss: 0.1977, Val Loss: 18.5724, Val Acc: 19.65%\n",
      "Epoch [60/200] - Train Loss: 0.1728, Val Loss: 19.9194, Val Acc: 20.01%\n",
      "Epoch [70/200] - Train Loss: 0.1590, Val Loss: 21.2632, Val Acc: 19.95%\n",
      "Epoch [80/200] - Train Loss: 0.1489, Val Loss: 22.3008, Val Acc: 19.58%\n",
      "Epoch [90/200] - Train Loss: 0.1426, Val Loss: 23.2862, Val Acc: 19.32%\n",
      "Epoch [100/200] - Train Loss: 0.1373, Val Loss: 24.2701, Val Acc: 19.14%\n",
      "Epoch [110/200] - Train Loss: 0.1333, Val Loss: 25.1455, Val Acc: 19.36%\n",
      "Epoch [120/200] - Train Loss: 0.1291, Val Loss: 26.2009, Val Acc: 19.13%\n",
      "Epoch [130/200] - Train Loss: 0.1273, Val Loss: 26.9540, Val Acc: 18.90%\n",
      "Epoch [140/200] - Train Loss: 0.1239, Val Loss: 27.8106, Val Acc: 18.54%\n",
      "Epoch [150/200] - Train Loss: 0.1219, Val Loss: 28.7098, Val Acc: 18.12%\n",
      "Epoch [160/200] - Train Loss: 0.1204, Val Loss: 29.5961, Val Acc: 18.22%\n",
      "Epoch [170/200] - Train Loss: 0.1184, Val Loss: 30.5844, Val Acc: 18.03%\n",
      "Epoch [180/200] - Train Loss: 0.1168, Val Loss: 31.4713, Val Acc: 17.69%\n",
      "Epoch [190/200] - Train Loss: 0.1158, Val Loss: 32.1599, Val Acc: 18.08%\n",
      "Epoch [200/200] - Train Loss: 0.1150, Val Loss: 33.1206, Val Acc: 17.89%\n",
      "\n",
      "Final Results - Val Loss: 33.1206, Val Acc: 17.89%\n",
      "Saved artifacts for model_5_ctx5_emb32_relu\n",
      "\n",
      "Sample predictions for model_5_ctx5_emb32_relu:\n",
      "  Context: goto exit_err; if <UNK> >\n",
      "  True: <UNK> | Predicted: NULL)\n",
      "\n",
      "  Context: */ case AUDIT_ARCH: if (f->op\n",
      "  True: != | Predicted: >\n",
      "\n",
      "  Context: #include \"trace_output.h\" #ifdef CONFIG_BLK_DEV_IO_TRACE static\n",
      "  True: unsigned | Predicted: int\n",
      "\n",
      "\n",
      "############################################################\n",
      "# MODEL 6/8: Context=5, Embedding=32, Activation=tanh\n",
      "############################################################\n",
      "\n",
      "Creating sequences with context length 5...\n",
      "Total sequences: 759634\n",
      "Train samples: 683670, Val samples: 75964\n",
      "\n",
      "Model architecture:\n",
      "  Vocab size: 100000\n",
      "  Embedding dim: 32\n",
      "  Context length: 5\n",
      "  Hidden size: 1024\n",
      "  Activation: tanh\n",
      "  Total parameters: 105,864,864\n",
      "\n",
      "============================================================\n",
      "Training: model_6_ctx5_emb32_tanh\n",
      "============================================================\n",
      "Epoch [1/200] - Train Loss: 6.8648, Val Loss: 6.4427, Val Acc: 19.17%\n",
      "Epoch [10/200] - Train Loss: 1.7879, Val Loss: 9.0504, Val Acc: 22.45%\n",
      "Epoch [20/200] - Train Loss: 0.9911, Val Loss: 10.9563, Val Acc: 21.22%\n",
      "Epoch [30/200] - Train Loss: 0.7125, Val Loss: 12.5301, Val Acc: 20.41%\n",
      "Epoch [40/200] - Train Loss: 0.5972, Val Loss: 13.9625, Val Acc: 20.26%\n",
      "Epoch [50/200] - Train Loss: 0.5520, Val Loss: 15.0346, Val Acc: 19.49%\n",
      "Epoch [60/200] - Train Loss: 0.5284, Val Loss: 15.9821, Val Acc: 18.66%\n",
      "Epoch [70/200] - Train Loss: 0.5179, Val Loss: 16.7876, Val Acc: 18.37%\n",
      "Epoch [80/200] - Train Loss: 0.5082, Val Loss: 17.4839, Val Acc: 18.07%\n",
      "Epoch [90/200] - Train Loss: 0.5025, Val Loss: 18.0635, Val Acc: 18.06%\n",
      "Epoch [100/200] - Train Loss: 0.4914, Val Loss: 18.5299, Val Acc: 18.13%\n",
      "Epoch [110/200] - Train Loss: 0.4875, Val Loss: 18.9672, Val Acc: 17.68%\n",
      "Epoch [120/200] - Train Loss: 0.4780, Val Loss: 19.4965, Val Acc: 17.37%\n",
      "Epoch [130/200] - Train Loss: 0.4706, Val Loss: 19.9162, Val Acc: 17.31%\n",
      "Epoch [140/200] - Train Loss: 0.4651, Val Loss: 20.3213, Val Acc: 16.13%\n",
      "Epoch [150/200] - Train Loss: 0.4566, Val Loss: 20.6411, Val Acc: 16.00%\n",
      "Epoch [160/200] - Train Loss: 0.4494, Val Loss: 20.9749, Val Acc: 16.49%\n",
      "Epoch [170/200] - Train Loss: 0.4428, Val Loss: 21.3576, Val Acc: 16.00%\n",
      "Epoch [180/200] - Train Loss: 0.4346, Val Loss: 21.6210, Val Acc: 15.71%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
